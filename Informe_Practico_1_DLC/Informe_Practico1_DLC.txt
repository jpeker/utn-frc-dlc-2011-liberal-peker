Informe sobre pruebas realizadas.

Alumnos Liberal Rodrigo, Julian Peker. DLC 2011. 4K4


Listado de Pruebas realizadas.

Prueba 1 tamaño inicial de la tabla 11
Tamaño final de la tabla 2671
Cantidad de registro a cargar 1000
Cantidad de Registros Cargados 994
Tamaño del archivo 187 k
Tiempo 5 seg


Prueba 2 tamaño inicial de la tabla 501
Tamaño final de la 2539
Cantidad de registro a cargar 1000
Cantidad de Registros Cargados 995
Tamaño del archivo 178 k
Tiempo 5 seg


Prueba 3 tamaño inicial de la tabla 3001
Tamaño final de la tabla 3001
Cantidad de registro a cargar 1000
Cantidad de Registros Cargados 993
Tamaño del archivo 211 k
Tiempo 1 segundos


Prueba 4 tamaño inicial de la tabla 10001
Tamaño final de la tabla 10001
Cantidad de registro a cargar 1000
Cantidad de Registros Cargados 998
Tamaño del archivo 703 k
Tiempo 2 segundos


Prueba 5 tamaño inicial de la tabla 100001
Tamaño final de la tabla 100001
Cantidad de registro a cargar 1000
Cantidad de Registros Cargados 997
Tamaño del archivo 6,86mb
Tiempo 13 seg


Prueba 6 tamaño inicial de la tabla 1000001
Tamaño final de la tabla 1000001
Cantidad de registro a cargar 1000
Cantidad de Registros Cargados 990
Tamaño del archivo 68,6mb
Tiempo 2 min 6 seg


Prueba 7 tamaño inicial de la tabla 1000001
Tamaño final de la tabla 1000001
Cantidad de registro a cargar 1000
Cantidad de Registros Cargados 995
Tamaño del archivo 70,3mb
Tiempo 3 min 27 seg


Prueba 8 tamaño inicial de la tabla 1000001
Tamaño final de la tabla 1000001
Cantidad de registro a cargar 100001
Cantidad de Registros Cargados 99883
Tamaño del archivo 70,3mb
Tiempo 3 min 27 seg


Prueba 8 tamaño inicial de la tabla 1000001
Tamaño final de la tabla 1000001
Cantidad de registro a cargar 1000000
Cantidad de Registros Cargados 632644
Tamaño del archivo 105,4mb
Tiempo 24 min 1 seg


Prueba 9 tamaño inicial de la tabla 11
Tamaño final de la tabla 1760203
Cantidad de registro a cargar 1000000
Cantidad de Registros Cargados 632072
Tamaño del archivo 123,7 mb
Tiempo 44 min 32 seg


Prueba 10 tamaño inicial de la tabla 1000001
Tamaño final de la tabla 1000001
Cantidad de registro a cargar 100000
Cantidad de Registros Cargados 95209
Tamaño del archivo 73,1 mb
Tiempo 4 min 38 seg 





Análisis de prueba realizados:

1.	Es mejor siempre empezar con una tabla de tamaño pequeño para observar detenidamente el proceso.
2.	Con una tabla de tamaño pequeño y la cantidad de registros a cargar no es grande se tienen los siguientes beneficios:
	a.	Menor tiempo de carga.
	b.	Archivo más pequeño (es decir un menor volumen de datos 		innecesarios).
	c.	El tamaño de la tabla se ajusta más a la cantidad de 		registros 	añadidos.
	d.	A veces la redispersion es necesaria, igual teniendo una 		tabla 	pequeña se ahorra más redispersando que generando una tabla 		de mayor tamaño.
3.	Pero a su vez existen las siguientes desventajas si la cantidad de registros a cargar se lleva a grandes valores y el tamaño de la tabla sigue siendo pequeño:
	a.	El archivo al realizar tantos rehashes se demora más en 	cargar sus datos. 
	b.	El rehash siempre termina generando un mayor archivo de la 	tabla cuando crece. Por otra parte tener una tabla de mayor tamaño 	prevendría generar rehashes innecesarios y a su vez controlar el 	tamaño de la tabla dejando un tamaño fijo.
4.	Es cierto también que existe la posibilidad de generar una tabla grande desde el inicio, pero es necesario conocer la cantidad de registros que la tabla contendrá grabados, porque en caso de ser una tabla grande el proceso de generación de la tabla es mucho mayor al proceso de inserción de datos, a pesar de no existir rehashes.
5.	De igual manera cuando tabla sea grande y haya que realizar un nuevo rehash esto va a generar una gran cantidad de tiempo en realizar el rehash de la tabla y también mayor espacio en disco cuando se genere el archivo temporal. El rehash hay que evitarlo cuando el volumen de datos empieza a crecer.
6.	También hay que tener en cuenta que se puede aplicar encadenamiento para el manejo de archivos, pero que sucede al basarse en linked list podrían generarse una gran cantidad de listas repetidas, aumentando el volumen de datos, mientras que a veces es mejor conocer el volumen de datos empleado rehashing, aunque la probabilidad de colisiones podría ser mayor.




Conclusión: 

•	Es mejor realizar redispersion antes que crear una tabla de mayor tamaño en el caso de que la tabla sea de tamaño inicial pequeño y la cantidad de registros sea limitada.
•	A grandes tablas, se recomienda insertar una mayor cantidad de registros, pero sin superar al rehash, porque se genera un mayor tiempo de procesamiento y espacio temporal en disco.
•	Es mas considerable una tabla de mayor tamaño destinada a una gran cantidad de registros que una de menor tamaño que deba realizar una gran cantidad de veces el rehash, porque el mismo tiende a generar mayor tiempo de procesamiento.
•	Seria interesante buscar la forma de generar un algoritmo predictivo que permita determinar o cantidad de inserciones posibles o ver si es necesario o no realizar un rehasing.




TroubleShooting generales que se presentaron:

En nuestro caso empleamos números aleatorios para ir insertando los distintos legajos de los alumnos (en este caso, podría haber sido cualquier otro tipo de objeto). A fin de cuentas salio un corolario de este práctico y es necesario que java desarrolle un poco mejor su generador de números aleatorios. Más de una vez se presento una colisión por tener el mismo legajo durante la ejecución de las pruebas. También podríamos haber empleado 2 números aleatorios para generar el legajo, tal vez eso reducirá las probabilidades de colisión. Como comentario personal del grupo podemos decir que es recomendable usar el generador aleatorio que provee GCC, ya que su probabilidad de colisión de aleatorios es más reducida. También es evidente que java no es un lenguaje que fue pensado para fines científicos de computación, tal como sea tornado C.




Preguntas destinadas al profesor:

¿Anduvimos leyendo un tema para el manejo de colisiones tablas hash que se llama encadenamiento con OverFlow Area, seria mas eficiente que el rehashing o el encadenamiento normal? http://www.eee.metu.edu.tr/~vision/LectureNotes/EE441/Chapt7.html

¿Nos falto considerar algo durante las pruebas?


